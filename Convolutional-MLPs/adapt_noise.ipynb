{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from ipywidgets import IntProgress\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import sys\n",
    "sys.argv = ['', \"-c\", \"configs/classification/finetuned/convmlp_m_cifar100.yml\", \"--resume\", \"chkpt/convmlp_m_cifar100.pth\", \"--download\", \"--data_dir\", \"dataset\"]\n",
    "\n",
    "from classification import config_parser, _parse_args, _logger\n",
    "import argparse\n",
    "import time\n",
    "import yaml\n",
    "import os\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from contextlib import suppress\n",
    "from datetime import datetime\n",
    "from importlib import import_module\n",
    "\n",
    "from timm.data import create_dataset, create_loader, resolve_data_config, Mixup, FastCollateMixup, AugMixDataset\n",
    "from timm.models import create_model, safe_model_name, resume_checkpoint, load_checkpoint, \\\n",
    "    convert_splitbn_model, model_parameters\n",
    "from timm.utils import *\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy, JsdCrossEntropy\n",
    "from timm.optim import create_optimizer_v2, optimizer_kwargs\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.utils import ApexScaler, NativeScaler\n",
    "\n",
    "from src.classification import *\n",
    "\n",
    "try:\n",
    "    from apex import amp\n",
    "    from apex.parallel import DistributedDataParallel as ApexDDP\n",
    "    from apex.parallel import convert_syncbn_model\n",
    "\n",
    "    has_apex = True\n",
    "except ImportError:\n",
    "    has_apex = False\n",
    "\n",
    "has_native_amp = False\n",
    "try:\n",
    "    if getattr(torch.cuda.amp, 'autocast') is not None:\n",
    "        has_native_amp = True\n",
    "except AttributeError:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FedCIAFR100(Dataset):\n",
    "    def __init__(self, path, idx_per_worker=50, total_worker=5, worker_idx=0)->None:\n",
    "        dataset = h5py.File(path, 'r')\n",
    "        total_idx = len(dataset['examples'].keys())\n",
    "        assert idx_per_worker * total_worker <= total_idx and worker_idx < total_worker\n",
    "        all_idx = random.sample(sorted(dataset['examples'].keys()), k=idx_per_worker * total_worker)\n",
    "        local_idx = all_idx[worker_idx * idx_per_worker: (worker_idx + 1) * idx_per_worker]\n",
    "        _logger.info(f'Local idx: {local_idx}')\n",
    "        self.x = np.vstack([dataset['examples'][idx]['image'][()] for idx in local_idx])\n",
    "        self.y = np.vstack([dataset['examples'][idx]['label'][()][:, None] for idx in local_idx]).squeeze()\n",
    "    \n",
    "    def __len__(self)->int:\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple:\n",
    "        img, target = self.x[index], self.y[index]\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "        epoch, model, loader, optimizer, loss_fn, args,\n",
    "        lr_scheduler=None, saver=None, output_dir=None, amp_autocast=suppress,\n",
    "        loss_scaler=None, model_ema=None, mixup_fn=None):\n",
    "    if args.mixup_off_epoch and epoch >= args.mixup_off_epoch:\n",
    "        if args.prefetcher and loader.mixup_enabled:\n",
    "            loader.mixup_enabled = False\n",
    "        elif mixup_fn is not None:\n",
    "            mixup_fn.mixup_enabled = False\n",
    "\n",
    "    second_order = hasattr(optimizer, 'is_second_order') and optimizer.is_second_order\n",
    "    batch_time_m = AverageMeter()\n",
    "    data_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    num_updates = epoch * len(loader)\n",
    "    for batch_idx, (input, target) in enumerate(loader):\n",
    "        last_batch = batch_idx == last_idx\n",
    "        data_time_m.update(time.time() - end)\n",
    "        if not args.prefetcher:\n",
    "            input, target = input.cuda(), target.cuda()\n",
    "            if mixup_fn is not None:\n",
    "                input, target = mixup_fn(input, target)\n",
    "        if args.channels_last:\n",
    "            input = input.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "        with amp_autocast():\n",
    "            output = model(input)\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "        if not args.distributed:\n",
    "            losses_m.update(loss.item(), input.size(0))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        if loss_scaler is not None:\n",
    "            loss_scaler(\n",
    "                loss, optimizer,\n",
    "                clip_grad=args.clip_grad, clip_mode=args.clip_mode,\n",
    "                parameters=model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                create_graph=second_order)\n",
    "        else:\n",
    "            loss.backward(create_graph=second_order)\n",
    "            if args.clip_grad is not None:\n",
    "                dispatch_clip_grad(\n",
    "                    model_parameters(model, exclude_head='agc' in args.clip_mode),\n",
    "                    value=args.clip_grad, mode=args.clip_mode)\n",
    "            optimizer.step()\n",
    "\n",
    "        if model_ema is not None:\n",
    "            model_ema.update(model)\n",
    "\n",
    "        torch.cuda.synchronize()\n",
    "        num_updates += 1\n",
    "        batch_time_m.update(time.time() - end)\n",
    "        if last_batch or batch_idx % args.log_interval == 0:\n",
    "            lrl = [param_group['lr'] for param_group in optimizer.param_groups]\n",
    "            lr = sum(lrl) / len(lrl)\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data, args.world_size)\n",
    "                losses_m.update(reduced_loss.item(), input.size(0))\n",
    "\n",
    "            if args.local_rank == 0:\n",
    "                _logger.info(\n",
    "                    'Train: {} [{:>4d}/{} ({:>3.0f}%)]  '\n",
    "                    'Loss: {loss.val:>9.6f} ({loss.avg:>6.4f})  '\n",
    "                    'Time: {batch_time.val:.3f}s, {rate:>7.2f}/s  '\n",
    "                    '({batch_time.avg:.3f}s, {rate_avg:>7.2f}/s)  '\n",
    "                    'LR: {lr:.3e}  '\n",
    "                    'Data: {data_time.val:.3f} ({data_time.avg:.3f})'.format(\n",
    "                        epoch,\n",
    "                        batch_idx, len(loader),\n",
    "                        100. * batch_idx / last_idx,\n",
    "                        loss=losses_m,\n",
    "                        batch_time=batch_time_m,\n",
    "                        rate=input.size(0) * args.world_size / batch_time_m.val,\n",
    "                        rate_avg=input.size(0) * args.world_size / batch_time_m.avg,\n",
    "                        lr=lr,\n",
    "                        data_time=data_time_m))\n",
    "\n",
    "                if args.save_images and output_dir:\n",
    "                    torchvision.utils.save_image(\n",
    "                        input,\n",
    "                        os.path.join(output_dir, 'train-batch-%d.jpg' % batch_idx),\n",
    "                        padding=0,\n",
    "                        normalize=True)\n",
    "\n",
    "        if saver is not None and args.recovery_interval and (\n",
    "                last_batch or (batch_idx + 1) % args.recovery_interval == 0):\n",
    "            saver.save_recovery(epoch, batch_idx=batch_idx)\n",
    "\n",
    "        if lr_scheduler is not None:\n",
    "            lr_scheduler.step_update(num_updates=num_updates, metric=losses_m.avg)\n",
    "\n",
    "        end = time.time()\n",
    "        # end for\n",
    "\n",
    "    if hasattr(optimizer, 'sync_lookahead'):\n",
    "        optimizer.sync_lookahead()\n",
    "\n",
    "    return OrderedDict([('loss', losses_m.avg)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, loader, loss_fn, args, amp_autocast=suppress, log_suffix=''):\n",
    "    batch_time_m = AverageMeter()\n",
    "    losses_m = AverageMeter()\n",
    "    top1_m = AverageMeter()\n",
    "    top5_m = AverageMeter()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    last_idx = len(loader) - 1\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input, target) in enumerate(loader):\n",
    "            last_batch = batch_idx == last_idx\n",
    "            if not args.prefetcher:\n",
    "                input = input.cuda()\n",
    "                target = target.cuda()\n",
    "            if args.channels_last:\n",
    "                input = input.contiguous(memory_format=torch.channels_last)\n",
    "\n",
    "            with amp_autocast():\n",
    "                output = model(input)\n",
    "            if isinstance(output, (tuple, list)):\n",
    "                output = output[0]\n",
    "\n",
    "            # augmentation reduction\n",
    "            reduce_factor = args.tta\n",
    "            if reduce_factor > 1:\n",
    "                output = output.unfold(0, reduce_factor, reduce_factor).mean(dim=2)\n",
    "                target = target[0:target.size(0):reduce_factor]\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "\n",
    "            if args.distributed:\n",
    "                reduced_loss = reduce_tensor(loss.data, args.world_size)\n",
    "                acc1 = reduce_tensor(acc1, args.world_size)\n",
    "                acc5 = reduce_tensor(acc5, args.world_size)\n",
    "            else:\n",
    "                reduced_loss = loss.data\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "            losses_m.update(reduced_loss.item(), input.size(0))\n",
    "            top1_m.update(acc1.item(), output.size(0))\n",
    "            top5_m.update(acc5.item(), output.size(0))\n",
    "\n",
    "            batch_time_m.update(time.time() - end)\n",
    "            end = time.time()\n",
    "            if args.local_rank == 0 and (last_batch or batch_idx % args.log_interval == 0):\n",
    "                log_name = 'Test' + log_suffix\n",
    "                _logger.info(\n",
    "                    '{0}: [{1:>4d}/{2}]  '\n",
    "                    'Time: {batch_time.val:.3f} ({batch_time.avg:.3f})  '\n",
    "                    'Loss: {loss.val:>7.4f} ({loss.avg:>6.4f})  '\n",
    "                    'Acc@1: {top1.val:>7.4f} ({top1.avg:>7.4f})  '\n",
    "                    'Acc@5: {top5.val:>7.4f} ({top5.avg:>7.4f})'.format(\n",
    "                        log_name, batch_idx, last_idx, batch_time=batch_time_m,\n",
    "                        loss=losses_m, top1=top1_m, top5=top5_m))\n",
    "\n",
    "    metrics = OrderedDict([('loss', losses_m.avg), ('top1', top1_m.avg), ('top5', top5_m.avg)])\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeither APEX or native Torch AMP is available, using float32. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstall NVIDA apex or upgrade to PyTorch 1.6\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m random_seed(args\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_connect_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_connect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# DEPRECATED, usedrop_path\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_block_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_block\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_momentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_momentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_eps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn_eps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscriptable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorchscript\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m param_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([m\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters()])\n\u001b[1;32m     37\u001b[0m data_config \u001b[38;5;241m=\u001b[39m resolve_data_config(\u001b[38;5;28mvars\u001b[39m(args), model\u001b[38;5;241m=\u001b[39mmodel, verbose\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/timm/models/factory.py:74\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnknown model (\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m model_name)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m---> 74\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m     77\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/classification/__init__.py:12\u001b[0m, in \u001b[0;36mconvmlp_m_classification\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvmlp_m_classification\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvmlp_m\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/convmlp.py:101\u001b[0m, in \u001b[0;36mconvmlp_m\u001b[0;34m(pretrained, progress, classifier_head, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvmlp_m\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, classifier_head\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconvmlp_m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mchannels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_conv_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifier_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_head\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/convmlp.py:84\u001b[0m, in \u001b[0;36m_convmlp\u001b[0;34m(arch, pretrained, progress, classifier_head, blocks, dims, mlp_ratios, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convmlp\u001b[39m(arch, pretrained, progress, classifier_head, blocks, dims, mlp_ratios, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 84\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mConvMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmlp_ratios\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratios\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mclassifier_head\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclassifier_head\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained \u001b[38;5;129;01mand\u001b[39;00m arch \u001b[38;5;129;01min\u001b[39;00m model_urls:\n\u001b[1;32m     87\u001b[0m         state_dict \u001b[38;5;241m=\u001b[39m load_state_dict_from_url(model_urls[arch],\n\u001b[1;32m     88\u001b[0m                                               progress\u001b[38;5;241m=\u001b[39mprogress)\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/convmlp.py:39\u001b[0m, in \u001b[0;36mConvMLP.__init__\u001b[0;34m(self, blocks, dims, mlp_ratios, channels, n_conv_blocks, classifier_head, num_classes, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(blocks)):\n\u001b[0;32m---> 39\u001b[0m     stage \u001b[38;5;241m=\u001b[39m \u001b[43mBasicStage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_blocks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m                       \u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdims\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmlp_ratios\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mstochastic_depth_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdownsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstages\u001b[38;5;241m.\u001b[39mappend(stage)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m classifier_head:\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/utils/modules.py:119\u001b[0m, in \u001b[0;36mBasicStage.__init__\u001b[0;34m(self, num_blocks, embedding_dims, mlp_ratio, stochastic_depth_rate, downsample)\u001b[0m\n\u001b[1;32m    117\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, stochastic_depth_rate, num_blocks)]\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_blocks):\n\u001b[0;32m--> 119\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mConvMLPStage\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedding_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdim_feedforward\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43membedding_dims\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mstochastic_depth_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m                         \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks\u001b[38;5;241m.\u001b[39mappend(block)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample_mlp \u001b[38;5;241m=\u001b[39m ConvDownsample(embedding_dims[\u001b[38;5;241m0\u001b[39m], embedding_dims[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m downsample \u001b[38;5;28;01melse\u001b[39;00m Identity()\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/utils/modules.py:81\u001b[0m, in \u001b[0;36mConvMLPStage.__init__\u001b[0;34m(self, embedding_dim, dim_feedforward, stochastic_depth_rate)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mlp1 \u001b[38;5;241m=\u001b[39m Mlp(embedding_dim_in\u001b[38;5;241m=\u001b[39membedding_dim, hidden_dim\u001b[38;5;241m=\u001b[39mdim_feedforward)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m LayerNorm(embedding_dim)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect \u001b[38;5;241m=\u001b[39m Conv2d(embedding_dim,\n\u001b[1;32m     79\u001b[0m                       embedding_dim,\n\u001b[1;32m     80\u001b[0m                       kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m---> 81\u001b[0m                       stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     82\u001b[0m                       padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     83\u001b[0m                       groups\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     84\u001b[0m                       bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect_norm \u001b[38;5;241m=\u001b[39m LayerNorm(embedding_dim)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mlp2 \u001b[38;5;241m=\u001b[39m Mlp(embedding_dim_in\u001b[38;5;241m=\u001b[39membedding_dim, hidden_dim\u001b[38;5;241m=\u001b[39mdim_feedforward)\n",
      "File \u001b[0;32m/Models/Convolutional-MLPs/src/utils/modules.py:81\u001b[0m, in \u001b[0;36mConvMLPStage.__init__\u001b[0;34m(self, embedding_dim, dim_feedforward, stochastic_depth_rate)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mlp1 \u001b[38;5;241m=\u001b[39m Mlp(embedding_dim_in\u001b[38;5;241m=\u001b[39membedding_dim, hidden_dim\u001b[38;5;241m=\u001b[39mdim_feedforward)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2 \u001b[38;5;241m=\u001b[39m LayerNorm(embedding_dim)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect \u001b[38;5;241m=\u001b[39m Conv2d(embedding_dim,\n\u001b[1;32m     79\u001b[0m                       embedding_dim,\n\u001b[1;32m     80\u001b[0m                       kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m),\n\u001b[0;32m---> 81\u001b[0m                       stride\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     82\u001b[0m                       padding\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m     83\u001b[0m                       groups\u001b[38;5;241m=\u001b[39membedding_dim,\n\u001b[1;32m     84\u001b[0m                       bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconnect_norm \u001b[38;5;241m=\u001b[39m LayerNorm(embedding_dim)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_mlp2 \u001b[38;5;241m=\u001b[39m Mlp(embedding_dim_in\u001b[38;5;241m=\u001b[39membedding_dim, hidden_dim\u001b[38;5;241m=\u001b[39mdim_feedforward)\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1363\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:662\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1288\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1250\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:297\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/debugpy/_vendored/pydevd/pydevd.py:1976\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   1973\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001b[0;32m-> 1976\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1978\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   1981\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/debugpy/_vendored/pydevd/pydevd.py:2011\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_mpl_hook()\n\u001b[1;32m   2010\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2011\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2013\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2015\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "setup_default_logging()\n",
    "args, args_text = _parse_args()\n",
    "\n",
    "args.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "use_amp = None\n",
    "if args.amp:\n",
    "    # `--amp` chooses native amp before apex (APEX ver not actively maintained)\n",
    "    if has_native_amp:\n",
    "        args.native_amp = True\n",
    "    elif has_apex:\n",
    "        args.apex_amp = True\n",
    "if args.apex_amp and has_apex:\n",
    "    use_amp = 'apex'\n",
    "elif args.native_amp and has_native_amp:\n",
    "    use_amp = 'native'\n",
    "elif args.apex_amp or args.native_amp:\n",
    "    _logger.warning(\"Neither APEX or native Torch AMP is available, using float32. \"\n",
    "                    \"Install NVIDA apex or upgrade to PyTorch 1.6\")\n",
    "random_seed(args.seed)\n",
    "\n",
    "model = create_model(\n",
    "    args.model,\n",
    "    pretrained=args.pretrained,\n",
    "    num_classes=args.num_classes,\n",
    "    drop_rate=args.drop,\n",
    "    drop_connect_rate=args.drop_connect,  # DEPRECATED, usedrop_path\n",
    "    drop_path_rate=args.drop_path,\n",
    "    drop_block_rate=args.drop_block,\n",
    "    global_pool=args.gp,\n",
    "    bn_tf=args.bn_tf,\n",
    "    bn_momentum=args.bn_momentum,\n",
    "    bn_eps=args.bn_eps,\n",
    "    scriptable=args.torchscript,\n",
    "    checkpoint_path=args.initial_checkpoint)\n",
    "\n",
    "param_count = sum([m.numel() for m in model.parameters()])\n",
    "data_config = resolve_data_config(vars(args), model=model, verbose=args.local_rank == 0)\n",
    "\n",
    "num_aug_splits = 0\n",
    "if args.aug_splits > 0:\n",
    "    assert args.aug_splits > 1, 'A split of 1 makes no sense'\n",
    "    num_aug_splits = args.aug_splits\n",
    "\n",
    "# enable split bn (separate bn stats per batch-portion)\n",
    "if args.split_bn:\n",
    "    assert num_aug_splits > 1 or args.resplit\n",
    "    model = convert_splitbn_model(model, max(num_aug_splits, 2))\n",
    "\n",
    "model.to(args.device)\n",
    "optimizer = create_optimizer_v2(model, **optimizer_kwargs(cfg=args))\n",
    "amp_autocast = None  # do nothing\n",
    "loss_scaler = None\n",
    "if use_amp == 'apex':\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level='O1')\n",
    "    loss_scaler = ApexScaler()\n",
    "    if args.local_rank == 0:\n",
    "        _logger.info('Using NVIDIA APEX AMP. Training in mixed precision.')\n",
    "elif use_amp == 'native':\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    loss_scaler = NativeScaler()\n",
    "    if args.local_rank == 0:\n",
    "        _logger.info('Using native Torch AMP. Training in mixed precision.')\n",
    "else:\n",
    "    if args.local_rank == 0:\n",
    "        _logger.info('AMP not enabled. Training in float32.')\n",
    "\n",
    "resume_epoch = None\n",
    "if args.resume:\n",
    "    resume_epoch = resume_checkpoint(\n",
    "        model, args.resume,\n",
    "        optimizer=None if args.no_resume_opt else optimizer,\n",
    "        loss_scaler=None if args.no_resume_opt else loss_scaler,\n",
    "        log_info=args.local_rank == 0)\n",
    "\n",
    "model_ema = None\n",
    "if args.model_ema:\n",
    "    # Important to create EMA model after cuda(), DP wrapper, and AMP but before SyncBN and DDP wrapper\n",
    "    model_ema = ModelEmaV2(\n",
    "        model, decay=args.model_ema_decay, device='cpu' if args.model_ema_force_cpu else None)\n",
    "    if args.resume:\n",
    "        load_checkpoint(model_ema.module, args.resume, use_ema=True)\n",
    "\n",
    "lr_scheduler, num_epochs = create_scheduler(args, optimizer)\n",
    "start_epoch = 0\n",
    "if args.start_epoch is not None:\n",
    "    # a specified start_epoch will always override the resume epoch\n",
    "    start_epoch = args.start_epoch\n",
    "elif resume_epoch is not None:\n",
    "    start_epoch = resume_epoch\n",
    "if lr_scheduler is not None and start_epoch > 0:\n",
    "    lr_scheduler.step(start_epoch)\n",
    "\n",
    "dataset_train = FedCIAFR100('datasets/image_blur_fed_cifar100_train.h5', total_worker=1, idx_per_worker=500, worker_idx=0)\n",
    "dataset_eval = FedCIAFR100('datasets/image_blur_fed_cifar100_test.h5', total_worker=1, idx_per_worker=100, worker_idx=0)\n",
    "\n",
    "# setup mixup / cutmix\n",
    "collate_fn = None\n",
    "mixup_fn = None\n",
    "mixup_active = args.mixup > 0 or args.cutmix > 0. or args.cutmix_minmax is not None\n",
    "if mixup_active:\n",
    "    mixup_args = dict(\n",
    "        mixup_alpha=args.mixup, cutmix_alpha=args.cutmix, cutmix_minmax=args.cutmix_minmax,\n",
    "        prob=args.mixup_prob, switch_prob=args.mixup_switch_prob, mode=args.mixup_mode,\n",
    "        label_smoothing=args.smoothing, num_classes=args.num_classes)\n",
    "    if args.prefetcher:\n",
    "        assert not num_aug_splits  # collate conflict (need to support deinterleaving in collate mixup)\n",
    "        collate_fn = FastCollateMixup(**mixup_args)\n",
    "    else:\n",
    "        mixup_fn = Mixup(**mixup_args)\n",
    "\n",
    "# wrap dataset in AugMix helper\n",
    "if num_aug_splits > 1:\n",
    "    dataset_train = AugMixDataset(dataset_train, num_splits=num_aug_splits)\n",
    "\n",
    "# create data loaders w/ augmentation pipeiine\n",
    "train_interpolation = args.train_interpolation\n",
    "if args.no_aug or not train_interpolation:\n",
    "    train_interpolation = data_config['interpolation']\n",
    "loader_train = create_loader(\n",
    "    dataset_train,\n",
    "    input_size=data_config['input_size'],\n",
    "    batch_size=args.batch_size,\n",
    "    is_training=True,\n",
    "    use_prefetcher=args.prefetcher,\n",
    "    no_aug=args.no_aug,\n",
    "    re_prob=args.reprob,\n",
    "    re_mode=args.remode,\n",
    "    re_count=args.recount,\n",
    "    re_split=args.resplit,\n",
    "    scale=args.scale,\n",
    "    ratio=args.ratio,\n",
    "    hflip=args.hflip,\n",
    "    vflip=args.vflip,\n",
    "    color_jitter=args.color_jitter,\n",
    "    auto_augment=args.aa,\n",
    "    num_aug_splits=num_aug_splits,\n",
    "    interpolation=train_interpolation,\n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=args.workers,\n",
    "    distributed=args.distributed,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=args.pin_mem,\n",
    "    use_multi_epochs_loader=args.use_multi_epochs_loader\n",
    ")\n",
    "\n",
    "loader_eval = create_loader(\n",
    "    dataset_eval,\n",
    "    input_size=data_config['input_size'],\n",
    "    batch_size=args.validation_batch_size_multiplier * args.batch_size,\n",
    "    is_training=False,\n",
    "    use_prefetcher=args.prefetcher,\n",
    "    interpolation=data_config['interpolation'],\n",
    "    mean=data_config['mean'],\n",
    "    std=data_config['std'],\n",
    "    num_workers=args.workers,\n",
    "    distributed=args.distributed,\n",
    "    crop_pct=data_config['crop_pct'],\n",
    "    pin_memory=args.pin_mem,\n",
    ")\n",
    "\n",
    "# setup loss function\n",
    "if args.jsd:\n",
    "    assert num_aug_splits > 1  # JSD only valid with aug splits set\n",
    "    train_loss_fn = JsdCrossEntropy(num_splits=num_aug_splits, smoothing=args.smoothing).cuda()\n",
    "elif mixup_active:\n",
    "    # smoothing is handled with mixup target transform\n",
    "    train_loss_fn = SoftTargetCrossEntropy().cuda()\n",
    "elif args.smoothing:\n",
    "    train_loss_fn = LabelSmoothingCrossEntropy(smoothing=args.smoothing).cuda()\n",
    "else:\n",
    "    train_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "validate_loss_fn = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# setup checkpoint saver and eval metric tracking\n",
    "eval_metric = args.eval_metric\n",
    "best_metric = None\n",
    "best_epoch = None\n",
    "saver = None\n",
    "output_dir = None\n",
    "if args.rank == 0:\n",
    "    if args.experiment:\n",
    "        exp_name = args.experiment\n",
    "    else:\n",
    "        exp_name = '-'.join([\n",
    "            datetime.now().strftime(\"%Y%m%d-%H%M%S\"),\n",
    "            safe_model_name(args.model),\n",
    "            str(data_config['input_size'][-1])\n",
    "        ])\n",
    "    output_dir = get_outdir(args.output if args.output else './output/train', exp_name)\n",
    "    decreasing = True if eval_metric == 'loss' else False\n",
    "    saver = CheckpointSaver(\n",
    "        model=model, optimizer=optimizer, args=args, model_ema=model_ema, amp_scaler=loss_scaler,\n",
    "        checkpoint_dir=output_dir, recovery_dir=output_dir, decreasing=decreasing, max_history=args.checkpoint_hist)\n",
    "    with open(os.path.join(output_dir, 'args.yaml'), 'w') as f:\n",
    "        f.write(args_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(start_epoch, num_epochs):\n",
    "    # train_metrics = OrderedDict([('loss', 0.)])\n",
    "    train_metrics = train_one_epoch(\n",
    "        epoch, model, loader_train, optimizer, train_loss_fn, args,\n",
    "        lr_scheduler=lr_scheduler, saver=saver, output_dir=output_dir,\n",
    "        amp_autocast=amp_autocast, loss_scaler=loss_scaler, model_ema=model_ema, mixup_fn=mixup_fn)\n",
    "\n",
    "    eval_metrics = validate(model, loader_eval, validate_loss_fn, args, amp_autocast=amp_autocast)\n",
    "\n",
    "    if model_ema is not None and not args.model_ema_force_cpu:\n",
    "        ema_eval_metrics = validate(\n",
    "            model_ema.module, loader_eval, validate_loss_fn, args, amp_autocast=amp_autocast,\n",
    "            log_suffix=' (EMA)')\n",
    "        eval_metrics = ema_eval_metrics\n",
    "\n",
    "    if lr_scheduler is not None:\n",
    "        # step LR for next epoch\n",
    "        lr_scheduler.step(epoch + 1, eval_metrics[eval_metric])\n",
    "\n",
    "    if output_dir is not None:\n",
    "        update_summary(\n",
    "            epoch, train_metrics, eval_metrics, os.path.join(output_dir, 'summary.csv'),\n",
    "            write_header=best_metric is None, log_wandb=args.log_wandb and has_wandb)\n",
    "\n",
    "    if saver is not None:\n",
    "        # save proper checkpoint with eval metric\n",
    "        save_metric = eval_metrics[eval_metric]\n",
    "        best_metric, best_epoch = saver.save_checkpoint(epoch, metric=save_metric)\n",
    "\n",
    "    if best_metric is not None:\n",
    "        _logger.info('*** Best metric: {0} (epoch {1})'.format(best_metric, best_epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar100_transform(img_mean, img_std, train = True, crop_size = (24,24)):\n",
    "    \"\"\"cropping, flipping, and normalizing.\"\"\"\n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomCrop(crop_size),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=img_mean, std=img_std),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.CenterCrop(crop_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=img_mean, std=img_std),\n",
    "        ])\n",
    "\n",
    "def preprocess_cifar_img(img, train):\n",
    "    # scale img to range [0,1] to fit ToTensor api\n",
    "    img = torch.div(img, 255.0)\n",
    "    transoformed_img = torch.stack([cifar100_transform\n",
    "        (i.type(torch.DoubleTensor).mean(),\n",
    "            i.type(torch.DoubleTensor).std(),\n",
    "            train)\n",
    "        (i.permute(2,0,1)) \n",
    "        for i in img])\n",
    "    return transoformed_img\n",
    "\n",
    "# fed_cifar100 is divided to totally 500 virtual workers, I assume each worker is allocated idx_per_worker virtual workers\n",
    "def prase_dataset(path, total_worker, train=True, idx_per_worker=50, batch_size=5, worker_idx=None, shuffle=True):\n",
    "    random.seed(2)\n",
    "    dataset = h5py.File(path, 'r')\n",
    "    total_idx = len(dataset['examples'].keys())\n",
    "    all_idx = random.sample(sorted(dataset['examples'].keys()), k=idx_per_worker * total_worker)\n",
    "    if worker_idx is None:\n",
    "        worker_idx = list(range(total_worker))\n",
    "    else:\n",
    "        worker_idx = [worker_idx]\n",
    "    all_dl = []\n",
    "    for _worker_idx in worker_idx:\n",
    "        local_idx = all_idx[_worker_idx * idx_per_worker: (_worker_idx + 1) * idx_per_worker]\n",
    "        x = np.vstack([dataset['examples'][idx]['image'][()] for idx in local_idx])\n",
    "        y = np.vstack([dataset['examples'][idx]['label'][()][:, None] for idx in local_idx]).squeeze()\n",
    "        \n",
    "        x = preprocess_cifar_img(torch.tensor(x), train=train)\n",
    "        y = torch.tensor(y)\n",
    "        ds = TensorDataset(x, y)\n",
    "        dl = DataLoader(ds, batch_size=batch_size, shuffle=shuffle)\n",
    "        all_dl.append(dl)\n",
    "    dataset.close()\n",
    "    return all_dl\n",
    "\n",
    "train_dl = prase_dataset('datasets/image_blur_fed_cifar100_train.h5', batch_size=100, train=True, total_worker=1, idx_per_worker=500)[0]\n",
    "test_dl = prase_dataset('datasets/image_blur_fed_cifar100_test.h5', batch_size=100, shuffle=False, train=False, total_worker=1, idx_per_worker=100)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False).to(device)\n",
    "chkpt = torch.load('chkpt/new_resnet18.chkpt')\n",
    "model.load_state_dict(chkpt)\n",
    "\n",
    "criterion = CrossEntropyLoss().to(device)\n",
    "optim = Adam(model.parameters(), lr=0.0001, weight_decay=0.0001, amsgrad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pretrained accuracy: 0.5137999653816223\n",
    "\n",
    "pretrained accuracy on increase brightness: 0.4854999780654907\n",
    "\n",
    "image blur: 0.2903999984264374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load('resnet18.chkpt'))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for data in test_dl:\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        output = model(x)\n",
    "        _, top_id = output.max(1)\n",
    "        correct += torch.sum(top_id == y)\n",
    "    correct_rate = correct / (len(test_dl) * test_dl.batch_size)\n",
    "    print(f'pretrained accuracy: {correct_rate}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./adapt_log')\n",
    "\n",
    "epoch_num = 100\n",
    "for i in tqdm(range(epoch_num), desc=\"Adapt epoch\"):\n",
    "    model.train()\n",
    "    log_loss = []\n",
    "    for idx, data in enumerate(train_dl):\n",
    "        x, y = data\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optim.zero_grad()\n",
    "        output = model(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        log_loss.append(loss.cpu())\n",
    "    log_loss = torch.mean(torch.tensor(log_loss))\n",
    "    writer.add_scalar('resnet18/loss', log_loss, i)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for data in test_dl:\n",
    "            x, y = data\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x)\n",
    "            _, top_id = output.max(1)\n",
    "            correct += torch.sum(top_id == y)\n",
    "        correct_rate = correct / (len(test_dl) * test_dl.batch_size)\n",
    "        writer.add_scalar('resnet18/test_accuracy', correct_rate, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'blur_resnet18.chkpt')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
